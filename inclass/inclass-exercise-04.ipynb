{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## PCA and K-means clustering with MNIST data\n\nFor the In-class exercise today we'll see how to project MNIST data onto a few principal components, and see if K-means clustering can correctly identify the data of the same digit. Let's load the MNIST data as before in the next cell.","metadata":{}},{"cell_type":"markdown","source":"### Add MNIST dataset to the noteook\n* First of all, click on \"+ Add data\" in the upper right corner, search for 'mnist npz', then click on the first one.\n* Run the first cell, which gives you the path of the MNIST dataset","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-09T02:32:53.630421Z","iopub.execute_input":"2022-03-09T02:32:53.631041Z","iopub.status.idle":"2022-03-09T02:32:53.666977Z","shell.execute_reply.started":"2022-03-09T02:32:53.630934Z","shell.execute_reply":"2022-03-09T02:32:53.665687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tip: If you want to look up what you've done in previous exercise/lab, you can expand the menu on the left hand side and click on 'Your Work'.","metadata":{}},{"cell_type":"code","source":"# Load the mnist dataset\nmnist = np.load('/kaggle/input/mnist-numpy/mnist.npz')\n#print what's inside the npz file\nprint(mnist.files)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T02:34:07.135176Z","iopub.execute_input":"2022-03-09T02:34:07.135467Z","iopub.status.idle":"2022-03-09T02:34:07.198247Z","shell.execute_reply.started":"2022-03-09T02:34:07.135438Z","shell.execute_reply":"2022-03-09T02:34:07.197462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the In-class exercise we'll focus on training data, and we'll use only 0's and 1's as before. Please pick 0's and 1's from the MNIST data as before.","metadata":{}},{"cell_type":"code","source":"# Choose 0's and 1's for x_train and y_train\n# Remember to normalize x_train by 255 (make the range from 0 to 1 for each pixel)\nx_train = mnist['x_train'][mnist['y_train']<=1]/255.\ny_train = mnist['y_train'][mnist['y_train']<=1]\nprint('x_train shape:', x_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T02:36:43.270761Z","iopub.execute_input":"2022-03-09T02:36:43.271127Z","iopub.status.idle":"2022-03-09T02:36:43.798779Z","shell.execute_reply.started":"2022-03-09T02:36:43.271071Z","shell.execute_reply":"2022-03-09T02:36:43.79789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, instead of trying to construct features (like the average pixel density) from the input, we simply treat the numbers associated with each pixel as a feature, i.e. each image has 28x28 = 784 features. It's a lot, no? So we will use only the first 3000 samples as the input for the rest of the exercise.\n\nIn the next cell, take the first 3000 samples from x_train and y_train, and use **reshape** to transform the shape into (3000, 784) for x_train.\n\n[Reference for np.reshpae](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html)","metadata":{}},{"cell_type":"code","source":"# Take the first 3000 samples\n# For each sample, reshape the 28*28 image into a 1D array of 28*28=784 entries\nx_train = np.reshape(x_train[:3000],(3000,784))\ny_train = y_train[:3000]\nprint('x_train shape after reshape:', x_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T02:42:23.790375Z","iopub.execute_input":"2022-03-09T02:42:23.790728Z","iopub.status.idle":"2022-03-09T02:42:23.797804Z","shell.execute_reply.started":"2022-03-09T02:42:23.790694Z","shell.execute_reply":"2022-03-09T02:42:23.796832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we use the method of PCA to reducd the dimension of the feature space from 784 to 2, and see how good we can separate 0's and 1's.","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# Try PCA with 2-component (reduction of dimension)\npca = PCA(n_components=2)\nPCA_components = pca.fit_transform(x_train)\n\n# Plot the output from the 2-component PCA\nfig = plt.figure(figsize=(10,6), dpi=80)  \nplt.scatter(PCA_components[:,0][y_train==0], PCA_components[:,1][y_train==0], alpha=.5, color='y', label='True 0')\nplt.scatter(PCA_components[:,0][y_train==1], PCA_components[:,1][y_train==1], alpha=.5, color='g', label='True 1')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T02:46:14.919202Z","iopub.execute_input":"2022-03-09T02:46:14.919478Z","iopub.status.idle":"2022-03-09T02:46:16.94623Z","shell.execute_reply.started":"2022-03-09T02:46:14.91945Z","shell.execute_reply":"2022-03-09T02:46:16.945331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like PCA has done a good job seperating 0's and 1's!\n\nIn the following, we pretend that we do not know the data come from 0's and 1's (imagine you are given the plot above with the same color for all points). We'll apply K-means on the distributions from PCA, and see if K-means can identify the correct two clusters.\n\nThe output of K-means is a 1D array of called **labels**, one number for each sample. Sample with the same label are identified as belonging to the same cluster.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n# Try K-means with 2 clusters\nkmeans = KMeans(n_clusters=2, random_state=0).fit(x_train)\nkmean_output = kmeans.labels_\nprint('KMeans labels shape:', kmean_output.shape)\n# Take a look at the first five entries of labels\nprint('KMeans labels look like:', kmean_output[0:6])","metadata":{"execution":{"iopub.status.busy":"2022-03-09T02:48:01.601551Z","iopub.execute_input":"2022-03-09T02:48:01.602247Z","iopub.status.idle":"2022-03-09T02:48:03.278685Z","shell.execute_reply.started":"2022-03-09T02:48:01.602195Z","shell.execute_reply":"2022-03-09T02:48:03.277933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the next cell, please plot the output from K-means on the place of PCA components, separating samples with different labels (use two different colors for the two labels). What do you find?","metadata":{}},{"cell_type":"code","source":"# Plot the output clusters by KMeans\nfig = plt.figure(figsize=(10,6), dpi=80)  \nplt.scatter(PCA_components[:,0][kmean_output==0], PCA_components[:,1][kmean_output==0], alpha=.5, color='y', label='Cluster 0')\nplt.scatter(PCA_components[:,0][kmean_output==1], PCA_components[:,1][kmean_output==1], alpha=.5, color='g', label='Cluster 1')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T02:58:29.542393Z","iopub.execute_input":"2022-03-09T02:58:29.542696Z","iopub.status.idle":"2022-03-09T02:58:29.815764Z","shell.execute_reply.started":"2022-03-09T02:58:29.542667Z","shell.execute_reply":"2022-03-09T02:58:29.814772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like we *almost* get the same clusters as before! Well, almost.\n\nPlease make a plot highlighting which points are grouped to the 'wrong' cluster. Does K-means make such mistake 'as expected'?","metadata":{}},{"cell_type":"code","source":"# Make a plot highlighting points that are grouped to the 'wrong' cluster by K-means using any method you like\nfig = plt.figure(figsize=(10,6), dpi=80) \n\nplt.scatter(PCA_components[:,0][kmean_output!=y_train], PCA_components[:,1][kmean_output!=y_train], alpha=.1, color='black', label='right')\nplt.scatter(PCA_components[:,0][kmean_output==y_train], PCA_components[:,1][kmean_output==y_train], alpha=.5, color='r', label='wrong')\nplt.xlabel('PCA 1')\nplt.ylabel('PCA 2')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T03:12:45.55579Z","iopub.execute_input":"2022-03-09T03:12:45.556105Z","iopub.status.idle":"2022-03-09T03:12:45.865672Z","shell.execute_reply.started":"2022-03-09T03:12:45.556057Z","shell.execute_reply":"2022-03-09T03:12:45.864489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we'll show how to make an 'elbow plot' to find the optimal k (number of clusters). We'd expect to see the turning point at k=2. Will that be the case?\n\nThe metric we'll use is the attribute **inertia** from Scikit-learn K-means, which is the sum of squared distances of samples to their closest cluster center. ","metadata":{}},{"cell_type":"code","source":"# Elbow plot\nks = range(1, 6)\ninertias = []\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k)\n    # Fit model to samples\n    model.fit(x_train)\n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n\n    \nfig = plt.figure(figsize=(6,6), dpi=80)\nplt.plot(ks, inertias, '-o', color='black')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T03:08:46.206736Z","iopub.execute_input":"2022-03-09T03:08:46.20761Z","iopub.status.idle":"2022-03-09T03:08:54.425242Z","shell.execute_reply.started":"2022-03-09T03:08:46.20756Z","shell.execute_reply":"2022-03-09T03:08:54.424346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}