{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## MNIST dataset with SVM, Decision Trees, and Random Forests\n\nThis week we're going to apply other classification algorithms on the MNIST data. As usual, click on **Add data** and search for 'mnist.npz', and add the MNIST data to this notebook. Then, run the first cell to show the path to the MNIST data.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-16T02:37:55.853582Z","iopub.execute_input":"2022-03-16T02:37:55.854382Z","iopub.status.idle":"2022-03-16T02:37:55.863773Z","shell.execute_reply.started":"2022-03-16T02:37:55.854331Z","shell.execute_reply":"2022-03-16T02:37:55.86313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At first we'll again focus on 0's and 1's. Please load the MNIST data, and pick 0's and 1's from  both x/y_train and x/y_test as before.","metadata":{}},{"cell_type":"code","source":"# Load the MNIST data\nmnist = np.load('/kaggle/input/mnist-numpy/mnist.npz')\n\n# Choose 0's and 1's for x_train, y_train, x_test, y_test\n# Remember to normalize x_train and x_test by 255 (make the range from 0 to 1 for each pixel)\nx_train = mnist['x_train'][mnist['y_train']<=1]/255\ny_train = mnist['y_train'][mnist['y_train']<=1]\nx_test = mnist['x_test'][mnist['y_test']<=1]/255\ny_test = mnist['y_test'][mnist['y_test']<=1]\nprint('x train shape:', x_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:40:11.231884Z","iopub.execute_input":"2022-03-16T02:40:11.232182Z","iopub.status.idle":"2022-03-16T02:40:11.588671Z","shell.execute_reply.started":"2022-03-16T02:40:11.232152Z","shell.execute_reply":"2022-03-16T02:40:11.587712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM with two features\n\nWe'll first use the two features we calculated before as inputs for the SVM: the average pixel density and the average density in the center of the image. (The 'center' is defined as the 6x8 pixels in the center of the 28x28 image.).\n\nPlease compute the two averages for each image in the x_train (for 0's and 1's), and produce a (new) x_train where each entry is (full average, center average). The shape of the resulting x_train should be (12665, 2). \n\nHint: We did this in the in-class exercise of Week 02 before!","metadata":{}},{"cell_type":"code","source":"# Prepare the training sample such that each entry is (full average, center average)\nx_train = np.array([[img.mean(),img[10:18, 11:17].mean()] for img in x_train])\nprint('x train shape becomes:', x_train.shape)\n\n# Please do the same for x_test\nx_test = np.array([[img.mean(),img[10:18, 11:17].mean()] for img in x_test])","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:46:58.127536Z","iopub.execute_input":"2022-03-16T02:46:58.128192Z","iopub.status.idle":"2022-03-16T02:46:58.494096Z","shell.execute_reply.started":"2022-03-16T02:46:58.128142Z","shell.execute_reply":"2022-03-16T02:46:58.493024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we'll create a linear SVM (SVM with a linear kernel) and check the performance of the classification from SVM. Just run the next cell.","metadata":{}},{"cell_type":"code","source":"# import svm from sklearn\nfrom sklearn import svm\n\n# Training with SVM with a linear kernal  C= regularization parameter\nclf = svm.SVC(kernel='linear', C=1.0)\nclf.fit(x_train, y_train)\ns_train = clf.score(x_train, y_train)\ns_test = clf.score(x_test, y_test)\nprint('Performance (training):', s_train)\nprint('Performance (testing):', s_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:09:09.195201Z","iopub.execute_input":"2022-03-16T03:09:09.195821Z","iopub.status.idle":"2022-03-16T03:09:09.840433Z","shell.execute_reply.started":"2022-03-16T03:09:09.195788Z","shell.execute_reply":"2022-03-16T03:09:09.839305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The performance is quite good! Only a few images in the test sample glot mis-identified. How many of them? How do they look like?\n\nIn the next cell, find how many figures are mis-identified in x_test. Furtheremore, plot a few of them (and better, label the true digit and the predicted digit, e.g. '0->1' means a true 0 that is mis-identified as 1).\n\nHint: use **np.where()**\n\nRef: [Usage of SVM classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) Check 'Methods'.","metadata":{}},{"cell_type":"code","source":"# Find mis-idenfied entries in x_test\n# You need to compare the prediction from the classifier (clf) and the ground truth (y_test)\nprint('Number of mis-identified figures: ', np.size(np.where(y_test!=clf.predict(x_test))))\n\n# Plot a couple of mis-identified images using any method you like\n# Label what the prediction and what the truth are\n\nfig = plt.figure(figsize=(3,5), dpi=80)\nfor i in range(6):\n#     misid = np.where(y_test!=clf.predict(x_test))[i]\n    plt.subplot(3,2,i+1)\n#     plt.title()\n#     plt.tick_params()\n    plt.imshow(x_test[y_test!=clf.predict(x_test)])","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:08:43.799411Z","iopub.execute_input":"2022-03-16T03:08:43.799702Z","iopub.status.idle":"2022-03-16T03:08:44.618549Z","shell.execute_reply.started":"2022-03-16T03:08:43.799673Z","shell.execute_reply":"2022-03-16T03:08:44.617999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can visualize the SVM 'border line' that separates 0's and 1's using a filled contour plot. Run the cell below to see the plot. Please make sure you understand what the code does for making this plot.","metadata":{}},{"cell_type":"code","source":"# Plotting the plane found by SVM on the all-mean v.s. center-mean 2D plane\nfig = plt.figure(figsize=(6,6), dpi=80)\n\n# 'Sampling points' on the 2D plane\nxv, yv = np.meshgrid(np.linspace(0.,0.45,100),np.linspace(-0.05,1.05,100))\n\n# ravel = reshape(-1) returns 1-D\n# np.c_: add the arrays along the 2nd axis \n# prediction made on each point of (all_mean, center_mean)\nzv = clf.predict(np.c_[xv.ravel(), yv.ravel()])\n\n# contourf: plot 'filled' contour\nplt.contourf(xv, yv, zv.reshape(xv.shape),alpha=.3, cmap='Blues')\n\n# overlay scatter plots of x_train data\nplt.scatter(x_train[:,0][y_train==0], x_train[:,1][y_train==0], c = 'y', s=5, alpha=0.8)\nplt.scatter(x_train[:,0][y_train==1], x_train[:,1][y_train==1], c = 'g', s=5, alpha=0.8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T02:56:41.195544Z","iopub.execute_input":"2022-03-16T02:56:41.19581Z","iopub.status.idle":"2022-03-16T02:56:41.680956Z","shell.execute_reply.started":"2022-03-16T02:56:41.195783Z","shell.execute_reply":"2022-03-16T02:56:41.679905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree with two features\n\nNow we build a decision tree to classify 0's and 1's with the two averages. We will use the confusion matrix on the **test sample** to judge its performance. Run the cell below to see what happens!","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\ndtree.fit(x_train , y_train)\n\n# Compare the prediction and the ground truth using x_test\ny_pred = dtree.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\ncmdtree = confusion_matrix(y_test,y_pred)\n# Print the confusion matrix\ncmdtree","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:09:56.365081Z","iopub.execute_input":"2022-03-16T03:09:56.365814Z","iopub.status.idle":"2022-03-16T03:09:56.47676Z","shell.execute_reply.started":"2022-03-16T03:09:56.365773Z","shell.execute_reply":"2022-03-16T03:09:56.475795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the cell below prints out the score (mean accuracy) of the decision tree on the training sample and on the test sample.\n\n[Usage of Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)","metadata":{}},{"cell_type":"code","source":"# Score of Decision Tree \nprint ('Performance on training data:', dtree.score(x_train, y_train))\nprint ('Performance on test data:', dtree.score(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:12:12.009039Z","iopub.execute_input":"2022-03-16T03:12:12.009703Z","iopub.status.idle":"2022-03-16T03:12:12.018553Z","shell.execute_reply.started":"2022-03-16T03:12:12.00966Z","shell.execute_reply":"2022-03-16T03:12:12.017521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You'll see that you reach 100% accuracy for the training data, i.e. the decision tree makes *no* mistakes for all the 0's and 1's in the training data. How does it happen? Please make a contour plot in the cell below showing the border line from the decision tree.","metadata":{}},{"cell_type":"code","source":"# Contour from decistion tree\n# Plotting the plane found by SVM on the all-mean v.s. center-mean 2D plane\nfig = plt.figure(figsize=(6,6), dpi=80)\n\n# 'Sampling points' on the 2D plane\nxv, yv = np.meshgrid(np.linspace(0.,0.45,100),np.linspace(-0.05,1.05,100))\n\n# ravel = reshape(-1) returns 1-D\n# np.c_: add the arrays along the 2nd axis \n# prediction made on each point of (all_mean, center_mean)\nzv = dtree.predict(np.c_[xv.ravel(), yv.ravel()])\n\n# contourf: plot 'filled' contour\nplt.contourf(xv, yv, zv.reshape(xv.shape),alpha=.3, cmap='Blues')\n\n# overlay scatter plots of x_train data\nplt.scatter(x_train[:,0][y_train==0], x_train[:,1][y_train==0], c = 'y', s=5, alpha=0.8)\nplt.scatter(x_train[:,0][y_train==1], x_train[:,1][y_train==1], c = 'g', s=5, alpha=0.8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:13:37.793094Z","iopub.execute_input":"2022-03-16T03:13:37.793398Z","iopub.status.idle":"2022-03-16T03:13:38.044584Z","shell.execute_reply.started":"2022-03-16T03:13:37.793368Z","shell.execute_reply":"2022-03-16T03:13:38.043644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You'll see the contour plot from the decision tree looks very 'unnatural', which implies the decision tree is overfitting the training data. Although it still gives a pretty good performance on the test sample of 0's and 1's, we'll see later an example of a decision tree fails to generalize to the test sample.","metadata":{}},{"cell_type":"markdown","source":"## Random Forest with two features\n\nWe claim an ensemble of decision trees can make the model less overfitting. Is that the case? Let's try with a Random Forest.\n\nIn the next cell we build a random forest model. Please print out the performance (score) on the training and on the test data, and make a contour plot in the following cell.","metadata":{}},{"cell_type":"code","source":"# Random Forest \nfrom sklearn.ensemble import RandomForestClassifier\nrforest = RandomForestClassifier()\nrforest.fit(x_train , y_train)\n\n# Print the scores of Random Forest\nprint ('Performance on training data:', rforest.score(x_train, y_train))\nprint ('Performance on test data:',rforest.score(x_test, y_test) )","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:16:10.186408Z","iopub.execute_input":"2022-03-16T03:16:10.187496Z","iopub.status.idle":"2022-03-16T03:16:11.347698Z","shell.execute_reply.started":"2022-03-16T03:16:10.187435Z","shell.execute_reply":"2022-03-16T03:16:11.346786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest Contour Plot\n# Contour from decistion tree\n# Plotting the plane found by SVM on the all-mean v.s. center-mean 2D plane\nfig = plt.figure(figsize=(6,6), dpi=80)\n\n# 'Sampling points' on the 2D plane\nxv, yv = np.meshgrid(np.linspace(0.,0.45,100),np.linspace(-0.05,1.05,100))\n\n# ravel = reshape(-1) returns 1-D\n# np.c_: add the arrays along the 2nd axis \n# prediction made on each point of (all_mean, center_mean)\nzv = rforest.predict(np.c_[xv.ravel(), yv.ravel()])\n\n# contourf: plot 'filled' contour\nplt.contourf(xv, yv, zv.reshape(xv.shape),alpha=.3, cmap='Blues')\n\n# overlay scatter plots of x_train data\nplt.scatter(x_train[:,0][y_train==0], x_train[:,1][y_train==0], c = 'y', s=5, alpha=0.8)\nplt.scatter(x_train[:,0][y_train==1], x_train[:,1][y_train==1], c = 'g', s=5, alpha=0.8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:16:32.963267Z","iopub.execute_input":"2022-03-16T03:16:32.96355Z","iopub.status.idle":"2022-03-16T03:16:33.267232Z","shell.execute_reply.started":"2022-03-16T03:16:32.96352Z","shell.execute_reply":"2022-03-16T03:16:33.266341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification of all digits with SVM, Decision Tree, and Random Forest\n\nWe now apply SVM, Decision Tree, and Random Forest to classify *all* digits (0-9) in the MNINST data! To save the time, we will use the first 10000 samples from the MNIST data.\n\nIn the next cell, we'll select the first 10000 samples, and treat all 784 pixels as features (using **reshape**) as we did last week.","metadata":{}},{"cell_type":"code","source":"# Select the first 10000 samples from x_train, y_train, x_test, y_test\nx_full_train = mnist['x_train'][:10000]/255.\nprint('x full train shape:', x_full_train.shape)\ny_full_train = mnist['y_train'][:10000]\nx_full_test = mnist['x_test']/255.\ny_full_test = mnist['y_test']\n\n# flatten input 28*28 images as 1-D arrays. The shape should be (10000, 784) for x_train and x_test.\nx_full_train = np.reshape(x_full_train,(10000,-1))\nprint('x full train shape now becomes:', x_full_train.shape)\nx_full_test = np.reshape(x_full_test,(10000,-1))","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:20:15.795325Z","iopub.execute_input":"2022-03-16T03:20:15.796112Z","iopub.status.idle":"2022-03-16T03:20:16.144388Z","shell.execute_reply.started":"2022-03-16T03:20:15.796069Z","shell.execute_reply":"2022-03-16T03:20:16.14346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Please fit the 'full_train' data with an SVM and compare the performances on the training and test samples.","metadata":{}},{"cell_type":"code","source":"# Create an SVM model and do the fit. Then compare the performances on the training and test samples\n\n# Training with SVM with a linear kernal  C= regularization parameter\nclf = svm.SVC(kernel='linear', C=1.0)\nclf.fit(x_full_train, y_full_train)\ns_train = clf.score(x_full_train, y_full_train)\ns_test = clf.score(x_full_test, y_full_test)\n\nprint('All-digit Performance (training):', s_train)\nprint('All-digit Performance (testing):', s_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:23:59.180757Z","iopub.execute_input":"2022-03-16T03:23:59.181599Z","iopub.status.idle":"2022-03-16T03:24:26.015219Z","shell.execute_reply.started":"2022-03-16T03:23:59.181553Z","shell.execute_reply":"2022-03-16T03:24:26.014586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks pretty good! What images (digits) are misidentified? Please plot a couple of mis-identified images from the x_full_test data, labeling the true and the predicted digits.","metadata":{}},{"cell_type":"code","source":"# Find and plot a couple of misidentified images from x_full_test\n# Label the predicted digit and the true digit\n\nunmatched = np.where(y_full_test!=clf.predict(x_full_test))\n\n# fig = plt.figure(figsize=(8,8), dpi=80)\n# for i in range(100):\n#     misid=unmatched[i]\n#     plt.subplot(10,10,i)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:30:34.029866Z","iopub.execute_input":"2022-03-16T03:30:34.030267Z","iopub.status.idle":"2022-03-16T03:30:44.226781Z","shell.execute_reply.started":"2022-03-16T03:30:34.030221Z","shell.execute_reply":"2022-03-16T03:30:44.225678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's fit the 10-digit data using a Decision Tree, and compare the performances on the training and test samples. What do you find?","metadata":{}},{"cell_type":"code","source":"# Create and fit a Decision Tree with all digits\n\ndtree.fit(x_full_train , y_full_train)\n\n# Compare the prediction and the ground truth using x_test\n#y_full_pred = dtree.predict(x_full_test)\n\n\n# Score of Decision Tree (should see the outcome of overtraining)\nprint('All-digit Performance (training):', dtree.score(x_full_train, y_full_train))\nprint('All-digit Performance (testing):', dtree.score(x_full_test, y_full_test))","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:35:17.387623Z","iopub.execute_input":"2022-03-16T03:35:17.388175Z","iopub.status.idle":"2022-03-16T03:35:19.500694Z","shell.execute_reply.started":"2022-03-16T03:35:17.388127Z","shell.execute_reply":"2022-03-16T03:35:19.500025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While we have a perfect performance on the training data, the accuracy is only about 80% for the test sample, which is clearly a sign of overfitting (sometimes called overtraining).\n\nWill using Random Forest be better? Let's find out in the next cell.","metadata":{}},{"cell_type":"code","source":"# Create and fit a Random Forest with all digits\nrforest = RandomForestClassifier()\nrforest.fit(x_full_train , y_full_train)\n\n# Score of Random Forest \nprint('All-digit Performance (training):', rforest.score(x_full_train, y_full_train))\nprint('All-digit Performance (testing):', rforest.score(x_full_test, y_full_test) )","metadata":{"execution":{"iopub.status.busy":"2022-03-16T03:36:29.42935Z","iopub.execute_input":"2022-03-16T03:36:29.429647Z","iopub.status.idle":"2022-03-16T03:36:35.443331Z","shell.execute_reply.started":"2022-03-16T03:36:29.42962Z","shell.execute_reply":"2022-03-16T03:36:35.442449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}