{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## MNIST dataset with Neural Networks!\n\nThis week we are going to play with NN using our old MNIST dataset! As usual please add the MNIST dataset and run the first cell to get its path.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname in os.listdir('/kaggle/input/phys591000-2022-week06/'):\n    print(dirname,\"/\")\n    for filename in os.listdir('/kaggle/input/phys591000-2022-week06/'+ dirname):\n        print(filename)\n    print(\"\\n\")\n\n    \n    \n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-23T02:43:40.158581Z","iopub.execute_input":"2022-03-23T02:43:40.158918Z","iopub.status.idle":"2022-03-23T02:43:40.174016Z","shell.execute_reply.started":"2022-03-23T02:43:40.158884Z","shell.execute_reply":"2022-03-23T02:43:40.172799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load and transform the MNIST dataset\n\nWe'll use the first 10000 samples from the MNIST dataset, and treat each pixel as a feature (i.e. we have 28x28=784 features). But this time we don't have to do anything with the x_train and x_test--we'll take care of the flatten/reshape when we build the NN model.\n\nOn the other hand, we need to translate the ground truth y_train and y_test from a single digit to one-hot encoding, i.e. \n0 -> \\[1,0,...,0], 1-> [0,1,0,..0],..., 9->[0,0,...,1]\n\n(The reason why that's necessary will be clearer later).  The code in the next cell shows how to do that using 'np.eye'. Please check that you understand how it works.\n","metadata":{}},{"cell_type":"code","source":"# Use only 10000 sample \n# Use np.eye for one-hot encoding\n# 0 -> [1,0,...,0], 1-> [0,1,0,..0],..., 9->[0,0,...,1] \n\nmnist = np.load('/kaggle/input/mnist-numpy/mnist.npz')\nx_train = mnist['x_train'][:10000]/255.\ny_train = np.array([np.eye(10)[n] for n in mnist['y_train'][:10000]])\nx_test = mnist['x_test']/255.\ny_test = np.array([np.eye(10)[n] for n in mnist['y_test']])\n\nprint('x_train shape is: ', x_train.shape)\nprint('y_train shape is: ', y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:43:44.084027Z","iopub.execute_input":"2022-03-23T02:43:44.084354Z","iopub.status.idle":"2022-03-23T02:43:44.584312Z","shell.execute_reply.started":"2022-03-23T02:43:44.08432Z","shell.execute_reply":"2022-03-23T02:43:44.583444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will build a NN model with 784 neurons in the input layer (since we have 784 features), one hidden layer with 30 neurons, and an output layer with 10 neurons (since we have 10 possible outcomes, 0-9). \n\nRun the cell below to see the performance. Pay attention to how we specifiy hyperparameters such as the activation functions (of each layer) and the loss function. **Please make sure you understand what each line in the code means.**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Reshape\nfrom tensorflow.keras.optimizers import SGD\n\nmodel = Sequential()\n# build a 784-30-10 model\nmodel = Sequential()\nmodel.add(Reshape((784,), input_shape=(28,28)))\nmodel.add(Dense(30, activation='sigmoid'))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=SGD(learning_rate=1.0),\n              metrics=['accuracy'])\n\nrec = model.fit(x_train, y_train, epochs=50, batch_size=100,\n                validation_data=(x_test, y_test))\n\n\nprint('Performance (training)')\nprint('Loss: %.5f, Acc: %.5f' % tuple(model.evaluate(x_train, y_train)))\nprint('Performance (testing)')\nprint('Loss: %.5f, Acc: %.5f' % tuple(model.evaluate(x_test, y_test)))                                                                       ","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:40:43.648526Z","iopub.execute_input":"2022-03-23T02:40:43.64883Z","iopub.status.idle":"2022-03-23T02:41:12.572242Z","shell.execute_reply.started":"2022-03-23T02:40:43.648797Z","shell.execute_reply":"2022-03-23T02:41:12.57135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A close look at the prediction from the NN\n\nAs before we'd like to see what images have fooled our NN. The first thing to check is exactly what the prediction looks like. In the next cell, print out the prediction for the first image in x_test, and compare that to the ground truth (y_test) of this image. What do you find?\n\nHint: The prediction can be fetched via model.predict('name of test sample').","metadata":{}},{"cell_type":"code","source":"# Print the first element of the prediction and compare to y_test\n\np_test = model.predict(x_test)\n\nprint('The first element of the prediction is: ',p_test[0])\nprint('The first element of the ground truth is: ',y_test[0])","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:53:07.302333Z","iopub.execute_input":"2022-03-23T02:53:07.302726Z","iopub.status.idle":"2022-03-23T02:53:07.781518Z","shell.execute_reply.started":"2022-03-23T02:53:07.302689Z","shell.execute_reply":"2022-03-23T02:53:07.780117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The prediction is an array of 10 numbers, all between 0 and 1. What do you think it means? How does that relate to the 'real' prediction is should make (i.e. a digit between 0 to 9)?\n\nIn the next cell, please plot (at least) 10 images that are wrongly classified by the NN. Please label the prediction and the ground truth.\n\nHint: Use np.argmax to compare the **indices** of the prediction and the y_test.","metadata":{}},{"cell_type":"code","source":"# Plot wrongly classified images using any method you like\n# Note: It's better to print ~10 images for later comparison\n\np_test_index = np.argmax(p_test, axis=1)\ny_test_index = np.argmax(y_test, axis=1)\nwrong_index  = np.where(p_test_index != y_test_index)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T03:09:05.408826Z","iopub.execute_input":"2022-03-23T03:09:05.409455Z","iopub.status.idle":"2022-03-23T03:09:05.415089Z","shell.execute_reply.started":"2022-03-23T03:09:05.4094Z","shell.execute_reply":"2022-03-23T03:09:05.414475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,6), dpi=200)\n\nfor i in range(10):\n    plt.subplot(2,5,i+1)\n    plt.imshow(x_test[wrong_index[0][i]])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T03:13:01.439023Z","iopub.execute_input":"2022-03-23T03:13:01.4393Z","iopub.status.idle":"2022-03-23T03:13:02.447357Z","shell.execute_reply.started":"2022-03-23T03:13:01.43927Z","shell.execute_reply":"2022-03-23T03:13:02.446239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To appreciate the 'probability' nature of the prediction from the softmax output layer, what if we manually choose the index corresponding to the second largest probability and interpret as the predicted digit? Will it happen to be the same as the ground truth?\n\nIn the next cell, compare the output from the second largest probability to the wrongly classified digits you've plotted above. Any '2nd choice' has made the right prediction?\n\nHint: You might find np.argsort useful for this case.","metadata":{}},{"cell_type":"code","source":"# Compare the output using 2nd largest probability to the ground truth\n# for wrongly classified digits\n\nsecond_pindex = np.argsort(p_test, axis=1)[:,-2]\n\nfig = plt.figure(figsize=(12,6), dpi=200)\n\nfor i in range(10):\n    plt.subplot(2,5,i+1)\n    plt.imshow(x_test[wrong_index[0][i]])\n    plt.title(\"real:\"+str(y_test_index[wrong_index[0][i]])+\" second:\"+str(second_pindex[wrong_index[0][i]]))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T03:28:08.145737Z","iopub.execute_input":"2022-03-23T03:28:08.146046Z","iopub.status.idle":"2022-03-23T03:28:09.291146Z","shell.execute_reply.started":"2022-03-23T03:28:08.14601Z","shell.execute_reply":"2022-03-23T03:28:09.28972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Overfitting and Regularization\n\nThe performance of the NN model we just built is superior with the training data, but slighly worse for the test data. This seems to be a sign of overtraining. Let's check the 'history' of the performance as a function of each epoch.\n\nThe cell below shows you how to plot the evolution of loss and accuracy from the NN model. Please make sure you understand the code.","metadata":{}},{"cell_type":"code","source":"# Plot showing the evolution of loss and accuracy, comparing the training and the test samples.\nfig = plt.figure(figsize=(6,6), dpi=80)\nplt.subplot(2,1,1)\nplt.plot(rec.history['loss'], lw=3, label='Train')\nplt.plot(rec.history['val_loss'], lw=3, label='Validation')\nplt.xlabel('epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.subplot(2,1,2)\nplt.plot(rec.history['accuracy'], lw=3, label='Train')\nplt.plot(rec.history['val_accuracy'], lw=3, label='Validation')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T03:24:17.203834Z","iopub.execute_input":"2022-03-23T03:24:17.204733Z","iopub.status.idle":"2022-03-23T03:24:17.534906Z","shell.execute_reply.started":"2022-03-23T03:24:17.20469Z","shell.execute_reply":"2022-03-23T03:24:17.533747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So indeeed while the performance with the training data improves along each epoch, the performance saturates much earlier for the test data. This means the model is overtrained/overfitting. \n\nIn the next cell we try to mitigate the overfitting by adding a regularization term at the output layer, using an 'L2' regularization.","metadata":{}},{"cell_type":"code","source":"# Regularization at output layer\nfrom tensorflow.keras.regularizers import l2\nm2 = Sequential()\nm2.add(Reshape((784,), input_shape=(28,28)))\nm2.add(Dense(30, activation='sigmoid'))\nm2.add(Dense(10, activation='softmax', kernel_regularizer=l2(0.01)))\n\nm2.compile(loss='categorical_crossentropy',\n              optimizer=SGD(learning_rate=1.0),\n              metrics=['accuracy'])\n\nrec2 = m2.fit(x_train, y_train, epochs=50, batch_size=100,\n                validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2022-03-23T03:28:31.752032Z","iopub.execute_input":"2022-03-23T03:28:31.752363Z","iopub.status.idle":"2022-03-23T03:28:53.801815Z","shell.execute_reply.started":"2022-03-23T03:28:31.752329Z","shell.execute_reply":"2022-03-23T03:28:53.801016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Please print out the performance of this regularized model on the training and on the test samples in the next cell.","metadata":{}},{"cell_type":"code","source":"# Print out performances on the training and the test data\n\n\nprint('Performance (training)')\nprint('Loss: %.5f, Acc: %.5f' % tuple(m2.evaluate(x_train, y_train)))\nprint('Performance (testing)')\nprint('Loss: %.5f, Acc: %.5f' % tuple(m2.evaluate(x_test, y_test)))      ","metadata":{"execution":{"iopub.status.busy":"2022-03-23T03:29:09.858564Z","iopub.execute_input":"2022-03-23T03:29:09.859099Z","iopub.status.idle":"2022-03-23T03:29:11.745658Z","shell.execute_reply.started":"2022-03-23T03:29:09.859061Z","shell.execute_reply":"2022-03-23T03:29:11.744772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the performances on the training and the test data are much consistent this time. Let's verify that's really the case by checking the historgy of this L2-regularized model.\n\nIn the next 2 cells please compare the evolution of loss and accuracy from the first (un-regularized) model and this L2-regularized one.","metadata":{}},{"cell_type":"code","source":"# Compare accuracy of the unregularized to the regularized one\n\n# Plot showing the evolution of loss and accuracy, comparing the training and the test samples.\nfig = plt.figure(figsize=(6,6), dpi=80)\nplt.subplot(2,1,1)\nplt.plot(rec.history['accuracy'], lw=3, label='Train unregularized')\nplt.plot(rec2.history['accuracy'], lw=3, label='Train regularized')\nplt.plot(rec.history['val_accuracy'], lw=3, label='Validation unregularized')\nplt.plot(rec2.history['val_accuracy'], lw=3, label='Validation regularized')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T03:36:26.373616Z","iopub.execute_input":"2022-03-23T03:36:26.37397Z","iopub.status.idle":"2022-03-23T03:36:26.607515Z","shell.execute_reply.started":"2022-03-23T03:36:26.373938Z","shell.execute_reply":"2022-03-23T03:36:26.606276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compare loss of the unregularized to the regularized one\n# Plot showing the evolution of loss and accuracy, comparing the training and the test samples.\nfig = plt.figure(figsize=(6,6), dpi=80)\nplt.subplot(2,1,1)\nplt.plot(rec.history['loss'], lw=3, label='Train unregularized')\nplt.plot(rec2.history['loss'], lw=3, label='Train regularized')\nplt.plot(rec.history['val_loss'], lw=3, label='Validation unregularized')\nplt.plot(rec2.history['val_loss'], lw=3, label='Validation regularized')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T03:36:58.333088Z","iopub.execute_input":"2022-03-23T03:36:58.333903Z","iopub.status.idle":"2022-03-23T03:36:58.565266Z","shell.execute_reply.started":"2022-03-23T03:36:58.33385Z","shell.execute_reply":"2022-03-23T03:36:58.564195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}