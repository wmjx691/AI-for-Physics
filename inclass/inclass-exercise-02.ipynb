{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PHYS591000 in 2022\n# inClass-exercise 02\n---","metadata":{}},{"cell_type":"markdown","source":"## Add MNIST dataset to the noteook\n\n1. First of all, click on \"+ Add data\" in the upper right corner, search for 'mnist npz', then click on the first one.\n2. Run the first cell, which gives you the path of the MNIST dataset\n\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n#import plotting\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-23T07:05:25.442842Z","iopub.execute_input":"2022-02-23T07:05:25.443125Z","iopub.status.idle":"2022-02-23T07:05:25.461504Z","shell.execute_reply.started":"2022-02-23T07:05:25.443096Z","shell.execute_reply":"2022-02-23T07:05:25.460621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load the MNIST file and compute the means\n\nLet's load the MNIST file. This time we'll pick the 0's and 1's in one go.  ","metadata":{}},{"cell_type":"code","source":"mnist = np.load('/kaggle/input/mnist-numpy/mnist.npz')\n\n# Prepare 0 and 1 in one single file\nx_train = mnist['x_train'][mnist['y_train']<=1]/255.\ny_train = mnist['y_train'][mnist['y_train']<=1]\n\n# Please check how many samples do we have in x_train and y_train\nprint(x_train.shape)\nprint(y_train.shape)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:05:25.463424Z","iopub.execute_input":"2022-02-23T07:05:25.46376Z","iopub.status.idle":"2022-02-23T07:05:26.11902Z","shell.execute_reply.started":"2022-02-23T07:05:25.463715Z","shell.execute_reply":"2022-02-23T07:05:26.118051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we calculate the means (average pixel densities). \nWe will use a slighly different expression this time, which will be useful later.","metadata":{}},{"cell_type":"code","source":"# Average pixel density for the full image\nx_train_fullavg = np.array([img.mean() for img in x_train])\n\n# As a check, make a plot the full average density for 0 and 1\n# Is it the same as your plot from Week 01 exercise?\nmean0 = x_train_fullavg[y_train==0]\nmean1 = x_train_fullavg[y_train==1]\n\nfig = plt.figure(figsize=(6,6), dpi=80)\nplt.hist(mean0, bins=50, color='y', label='mean0')\nplt.hist(mean1, bins=50, color='g', alpha=0.8, label='mean1')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:05:26.12067Z","iopub.execute_input":"2022-02-23T07:05:26.121087Z","iopub.status.idle":"2022-02-23T07:05:26.828947Z","shell.execute_reply.started":"2022-02-23T07:05:26.121044Z","shell.execute_reply":"2022-02-23T07:05:26.828267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we'll calculate one more feature, the average density in the center of the image. The 'center' is defined as the 6x8 pixels in the center of the 28x28 image.","metadata":{}},{"cell_type":"code","source":"# Calculate the 'center average'\n# Hint: Take the mean over a range in axis-0 and axis-1 for each 'img' in x_train\n# Hint2: For example, the range for axis-0 of each img is [10:18]\n\nx_train_cenavg = np.array([img[10:18,11:17].mean() for img in x_train])\n\n# As a check, make a plot the full average density for 0 and 1\n# Is it the same as your plot from Week 01 exercise?\ncen_mean0 = x_train_cenavg[y_train==0]\ncen_mean1 = x_train_cenavg[y_train==1]\n\n# Make a plot the center average density for 0 and 1\n# Does the plot make sense?\nfig = plt.figure(figsize=(6,6), dpi=80)\nplt.hist(cen_mean0, bins=50, color='y', label='mean0')\nplt.hist(cen_mean1, bins=50, color='g', alpha=0.8, label='mean1')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:05:26.83132Z","iopub.execute_input":"2022-02-23T07:05:26.831855Z","iopub.status.idle":"2022-02-23T07:05:27.52759Z","shell.execute_reply.started":"2022-02-23T07:05:26.831809Z","shell.execute_reply":"2022-02-23T07:05:27.526732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can compare the ROC curves for each feature using Scikit-learn. We need to compute ROC curves with **Test data**. \n\nROC curve computes the true positive rate (tpr) and false positive rate (fpr) for different thresholds (thr) on the 'score' of the classifier. In our case, the 'score' is simply the average pixel density. \n\n[Learn more about ROC curve in Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)","metadata":{}},{"cell_type":"code","source":"# Prepare the test samples of 0 and 1 as we've done for the training samples\n\nx_test = mnist['x_test'][mnist['y_test']<=1]/255.\ny_test = mnist['y_test'][mnist['y_test']<=1]\n\n# Import Scikit-learn ROC functions\nfrom sklearn.metrics import roc_curve, auc\n\n# ROC for full average.\n# The score is simply the full average. \n\ny_fullavg_score = np.array([img.mean() for img in x_test])\n\n# set pos_label=0 because 0's have higher full average \n# but 0 is defined as the background\nfpr_full, tpr_full, thr_full = roc_curve(y_test, y_fullavg_score, pos_label=0)\n# Calculate AUC\nroc_auc_full = auc(fpr_full, tpr_full)\nprint('AUC for full average is:',roc_auc_full)\n\n# Plot ROC\nplt.figure(figsize=(5,5))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr_full,tpr_full, color='red',label = 'All mean AUC = %0.2f' % roc_auc_full)\n# Add a diagonal line representing the ROC from random choice\nplt.plot([0, 1], [0, 1],linestyle='--', c='grey')\nplt.legend(loc = 'lower right')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:05:27.529311Z","iopub.execute_input":"2022-02-23T07:05:27.529915Z","iopub.status.idle":"2022-02-23T07:05:28.778703Z","shell.execute_reply.started":"2022-02-23T07:05:27.529859Z","shell.execute_reply":"2022-02-23T07:05:28.777859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Please plot the ROC curve from the center average and compare that to the one from the full average (plot both ROC curve on the same plot).","metadata":{}},{"cell_type":"code","source":"# Score from center average and compute ROC\ny_cenavg_score = np.array([img[10:18,11:17].mean() for img in x_test])\n\nfpr_cen, tpr_cen, thr_cen = roc_curve(y_test, y_cenavg_score)\n\n# Calculate AUC\nroc_auc_cen = auc(fpr_cen, tpr_cen)\nprint('AUC for center average is:',roc_auc_cen)\n\n# Plot ROC for the two averages on the same plot\nplt.figure(figsize=(5,5))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr_full,tpr_full, color='red',label = 'All mean AUC = %0.2f' % roc_auc_full)\nplt.plot(fpr_cen,tpr_cen, color='blue',label = 'Center mean AUC = %0.2f' % roc_auc_cen)\n# Add a diagonal line representing the ROC from random choice\nplt.plot([0, 1], [0, 1],linestyle='--', c='grey')\nplt.legend(loc = 'lower right')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:05:28.780146Z","iopub.execute_input":"2022-02-23T07:05:28.781065Z","iopub.status.idle":"2022-02-23T07:05:29.033257Z","shell.execute_reply.started":"2022-02-23T07:05:28.781017Z","shell.execute_reply":"2022-02-23T07:05:29.032166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So indeed the center average is another useful feature. Before combining the two features, let's take a closer look at the relations of the two kinds of averages.","metadata":{}},{"cell_type":"code","source":"# What is the correlation of the two features?\nR = np.corrcoef(x_train_fullavg, x_train_cenavg)\nprint(R)\n\n# Make a 2D plot of full average v.s. center average\nfig = plt.figure(figsize=(6,6), dpi=80)\nplt.scatter(mean0, cen_mean0, c='y', s=20, label='True 0')\nplt.scatter(mean1, cen_mean1, c='g', s=20, label='True 1')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:05:29.034625Z","iopub.execute_input":"2022-02-23T07:05:29.035506Z","iopub.status.idle":"2022-02-23T07:05:29.457836Z","shell.execute_reply.started":"2022-02-23T07:05:29.035455Z","shell.execute_reply":"2022-02-23T07:05:29.456986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LDA from Scikit-learn \n\nLet's use Scikit-learn to perform the LDA with the two averages. First we'll prepare the inputs for LDA: The training sample should be an array of \\[full average, center average\\] for each sample in the training data.","metadata":{}},{"cell_type":"code","source":"# Prepare the training sample such that each entry is [full average, center average]\nx_train_new = np.array([[img.mean(),img[10:18,11:17].mean()] for img in x_train])\n\n# Prepare the test sample as well\nx_test_new = np.array([[img.mean(),img[10:18,11:17].mean()] for img in x_test])\n\n\n# Check that the shape of the (new) x_train should be (12665, 2)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:05:29.459157Z","iopub.execute_input":"2022-02-23T07:05:29.459422Z","iopub.status.idle":"2022-02-23T07:05:29.896642Z","shell.execute_reply.started":"2022-02-23T07:05:29.459392Z","shell.execute_reply":"2022-02-23T07:05:29.895758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we import LDA from Scikit-learn. We will train the LDA, and compare the performance on the training and the test data.\n\n[Learn more about LDA in Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)","metadata":{}},{"cell_type":"code","source":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n# Training w/ LDA from skLearn\nclf = LinearDiscriminantAnalysis()\nf_train = clf.fit_transform(x_train_new, y_train)\n\n# Compare the performance of the LDA on the training and the test data\n# Are they similar?\ns_train = clf.score(x_train_new, y_train)\ns_test = clf.score(x_test_new, y_test)\nprint('Performance (training):', s_train)\nprint('Performance (testing):', s_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:05:29.907899Z","iopub.execute_input":"2022-02-23T07:05:29.908142Z","iopub.status.idle":"2022-02-23T07:05:30.0484Z","shell.execute_reply.started":"2022-02-23T07:05:29.908112Z","shell.execute_reply":"2022-02-23T07:05:30.047406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code below shows how the mapped distribution from LDA looks like:","metadata":{}},{"cell_type":"code","source":"# Plotting LDA results\nfig = plt.figure(figsize=(6,6), dpi=80)\nplt.hist(f_train[y_train==0], bins=50, color='y')\nplt.hist(f_train[y_train==1], bins=50, color='g', alpha=0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:05:30.0503Z","iopub.execute_input":"2022-02-23T07:05:30.050655Z","iopub.status.idle":"2022-02-23T07:05:30.56839Z","shell.execute_reply.started":"2022-02-23T07:05:30.050611Z","shell.execute_reply":"2022-02-23T07:05:30.567455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again we can plot the ROC curve along with the AUC from the LDA trained by Scikit-learn. Does it perform better than using a single feature?","metadata":{}},{"cell_type":"code","source":"# The score from LDA\ny_pred = clf.decision_function(x_test_new)\n\n# Calculate the AUC and make the ROC plot\nfpr_full, tpr_full, thr_full = roc_curve(y_test, y_pred, pos_label=1)\n# Calculate AUC\nroc_auc_full = auc(fpr_full, tpr_full)\nprint('AUC for full average is:',roc_auc_full)\n\n# Plot ROC\nplt.figure(figsize=(5,5))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr_full,tpr_full, color='red',label = 'All mean AUC = %0.2f' % roc_auc_full)\n# Add a diagonal line representing the ROC from random choice\nplt.plot([0, 1], [0, 1],linestyle='--', c='grey')\nplt.legend(loc = 'lower right')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.grid()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-23T07:05:30.569903Z","iopub.execute_input":"2022-02-23T07:05:30.57013Z","iopub.status.idle":"2022-02-23T07:05:30.784696Z","shell.execute_reply.started":"2022-02-23T07:05:30.570103Z","shell.execute_reply":"2022-02-23T07:05:30.783524Z"},"trusted":true},"execution_count":null,"outputs":[]}]}