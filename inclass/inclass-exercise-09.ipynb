{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## MNIST dataset with Convolution Neural Networks!\n\nThis week we are going to play with CNN using our old MNIST dataset! As usual please add the MNIST dataset and run the first cell to get its path.\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-13T02:37:39.527529Z","iopub.execute_input":"2022-04-13T02:37:39.52831Z","iopub.status.idle":"2022-04-13T02:37:39.664565Z","shell.execute_reply.started":"2022-04-13T02:37:39.528206Z","shell.execute_reply":"2022-04-13T02:37:39.663935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As for the exercise with NN, we'll use the first 10000 samples from the MNIST dataset, and translate the ground truth y_train and y_test from a single digit to one-hot encoding, i.e. 0 -> \\[1,0,...,0], 1-> [0,1,0,..0],..., 9->[0,0,...,1] using np.eye.  \n\nYou can check what we've done in Week 06.","metadata":{}},{"cell_type":"code","source":"# Use np.eye for one-hot encoding\n# 0 -> [1,0,...,0], 1-> [0,1,0,..0],..., 9->[0,0,...,1] \n\nmnist = np.load('/kaggle/input/mnist-numpy/mnist.npz')\nx_train = mnist['x_train'][:10000]/255.\ny_train = np.array([np.eye(10)[n] for n in mnist['y_train'][:10000]])\nx_test = mnist['x_test']/255.\ny_test = np.array([np.eye(10)[n] for n in mnist['y_test']])\n\nprint('x_train shape is: ', x_train.shape)\nprint('y_train shape is: ', y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T02:42:16.794539Z","iopub.execute_input":"2022-04-13T02:42:16.794818Z","iopub.status.idle":"2022-04-13T02:42:17.532703Z","shell.execute_reply.started":"2022-04-13T02:42:16.79479Z","shell.execute_reply":"2022-04-13T02:42:17.531654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we'll build a CNN model with 16 5x5 filters, 2x2 max pooling layer, flatten the max-pooled output and feed it into a fully-connected NN with 10 outputs (for digit 0-9).\n\nNote that we reshape the input to (28,28,1) for gray-scale images.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\n# Import all layers!\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import SGD, Adadelta\n\nmodel = Sequential()\n# build a CNN model, input shape channel = 1 for gray scale\nmodel.add(Reshape((28,28,1), input_shape=(28,28)))\nmodel.add(Conv2D(16, kernel_size=(5,5), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Flatten())\n#model.add(Dropout(0.1))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=Adadelta(),\n              metrics=['accuracy'])\n\n# Print model architecture\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T02:42:31.522413Z","iopub.execute_input":"2022-04-13T02:42:31.522689Z","iopub.status.idle":"2022-04-13T02:42:38.333697Z","shell.execute_reply.started":"2022-04-13T02:42:31.522658Z","shell.execute_reply":"2022-04-13T02:42:38.332707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's fit the model!","metadata":{}},{"cell_type":"code","source":"model.fit(x_train, y_train, epochs=100, batch_size=50,\n          validation_data=(x_test, y_test))\n\nprint('Performance (training)')\nprint('Loss: %.5f, Acc: %.5f' % tuple(model.evaluate(x_train, y_train)))\nprint('Performance (testing)')\nprint('Loss: %.5f, Acc: %.5f' % tuple(model.evaluate(x_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2022-04-13T02:42:49.152341Z","iopub.execute_input":"2022-04-13T02:42:49.152612Z","iopub.status.idle":"2022-04-13T02:45:02.104758Z","shell.execute_reply.started":"2022-04-13T02:42:49.152582Z","shell.execute_reply":"2022-04-13T02:45:02.103252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization of the filters \n\nHow do the filters and the feature maps look like? We can use the following code to fetch the weights (elements of the filters) and plot them. There are 16 5x5 filters in our model.","metadata":{}},{"cell_type":"code","source":"# Visualize the filters\n# Weights come in the form filters, biases = model.layers[1].get_weights()\nw = model.layers[1].get_weights()\nprint('weights from Conv2D layer shape is:', np.array(w[0]).shape)\nfig = plt.figure(figsize=(8,8), dpi=80)\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.imshow(w[0][:,:,:,i], cmap='Greys')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T02:45:02.106766Z","iopub.execute_input":"2022-04-13T02:45:02.107099Z","iopub.status.idle":"2022-04-13T02:45:03.438538Z","shell.execute_reply.started":"2022-04-13T02:45:02.107058Z","shell.execute_reply":"2022-04-13T02:45:03.437968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the plots above, the light squares are small weights and the dark squares represent large weights. So the filters pick up different kinds of patterns in the image.","metadata":{}},{"cell_type":"markdown","source":"## Visualization of the feature maps\n\nThe feature maps represent how the CNN 'sees' the input image. It will be interesting to see how the feature maps look like! In the code below, we first build a test model whose output is the output of the first hidden layer (the only Conv2D in our model), then we plot the output. There will be 16 feature maps for a single input image.\n\nWe'll use the first sample in the x_test as the input to this test model. What is the corresponding digit of the image?","metadata":{}},{"cell_type":"code","source":"# redefine model to output right after the first hidden layer\nfrom tensorflow.keras.models import Model\nmodel_test = Model(inputs=model.inputs, outputs=model.layers[1].output)\n\n# Use the 1st image in x_test as input\n# What is the digit this image represents?\nplt.imshow(x_test[0])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T02:55:03.757186Z","iopub.execute_input":"2022-04-13T02:55:03.758116Z","iopub.status.idle":"2022-04-13T02:55:03.88678Z","shell.execute_reply.started":"2022-04-13T02:55:03.758073Z","shell.execute_reply":"2022-04-13T02:55:03.885897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to first transform the shape of x_test\\[0] from (28,28) to (1,28,28),\nso that it looks like an input sample with only one image.\n\nUse any method you like to define an input 'img' of shape (1,28,28) from x_test\\[0].","metadata":{}},{"cell_type":"code","source":"# Add a new axis/dimension so that the shape of the input will be (1,28,28), \n# i.e. an input sample with only one image\nimg = x_test[0][np.newaxis]\nprint('img shape is:', img.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T02:59:27.065886Z","iopub.execute_input":"2022-04-13T02:59:27.066338Z","iopub.status.idle":"2022-04-13T02:59:27.071895Z","shell.execute_reply.started":"2022-04-13T02:59:27.066293Z","shell.execute_reply":"2022-04-13T02:59:27.071047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we take the prediction (output) of the input image from the test model and plot them.","metadata":{}},{"cell_type":"code","source":"# get feature map for first hidden layer\nfeature_maps = model_test.predict(img)\n#print('output shape is:', feature_maps.shape)\n\nfig = plt.figure(figsize=(8,8), dpi=80)\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.imshow(feature_maps[0,:,:,i], cmap='Greys')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T03:00:29.499903Z","iopub.execute_input":"2022-04-13T03:00:29.500694Z","iopub.status.idle":"2022-04-13T03:00:31.12619Z","shell.execute_reply.started":"2022-04-13T03:00:29.500647Z","shell.execute_reply":"2022-04-13T03:00:31.125486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, some feature maps capture a lot of details of the image, and some show much less details. This makes the CNN able to abstract more general concepts for classifcation of the images, and thus generalize to other samples.","metadata":{}}]}